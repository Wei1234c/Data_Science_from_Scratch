{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. [Getting Data 取得數據資料](https://github.com/joelgrus/data-science-from-scratch/blob/master/code/getting_data.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twython'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-39288912a366>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtwython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTwython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# fill these in if you want to use the code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'twython'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter\n",
    "import math, random, csv, json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "######\n",
    "#\n",
    "# BOOKS ABOUT DATA\n",
    "#\n",
    "######\n",
    "\n",
    "def is_video(td):\n",
    "    \"\"\"it's a video if it has exactly one pricelabel, and if\n",
    "    the stripped text inside that pricelabel starts with 'Video'\"\"\"\n",
    "    pricelabels = td('span', 'pricelabel')\n",
    "    return (len(pricelabels) == 1 and\n",
    "            pricelabels[0].text.strip().startswith(\"Video\"))\n",
    "\n",
    "def book_info(td):\n",
    "    \"\"\"given a BeautifulSoup <td> Tag representing a book,\n",
    "    extract the book's details and return a dict\"\"\"\n",
    "    \n",
    "    title = td.find(\"div\", \"thumbheader\").a.text\n",
    "    by_author = td.find('div', 'AuthorName').text\n",
    "    authors = [x.strip() for x in re.sub(\"^By \", \"\", by_author).split(\",\")]\n",
    "    isbn_link = td.find(\"div\", \"thumbheader\").a.get(\"href\")\n",
    "    isbn = re.match(\"/product/(.*)\\.do\", isbn_link).groups()[0]\n",
    "    date = td.find(\"span\", \"directorydate\").text.strip()\n",
    "    \n",
    "    return {\n",
    "        \"title\" : title,\n",
    "        \"authors\" : authors,\n",
    "        \"isbn\" : isbn,\n",
    "        \"date\" : date\n",
    "    }\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "def scrape(num_pages=31):\n",
    "    base_url = \"http://shop.oreilly.com/category/browse-subjects/\" + \\\n",
    "           \"data.do?sortby=publicationDate&page=\"\n",
    "\n",
    "    books = []\n",
    "\n",
    "    for page_num in range(1, num_pages + 1):\n",
    "        print(\"souping page\", page_num)\n",
    "        url = base_url + str(page_num)\n",
    "        soup = BeautifulSoup(requests.get(url).text, 'html5lib')\n",
    "            \n",
    "        for td in soup('td', 'thumbtext'):\n",
    "            if not is_video(td):\n",
    "                books.append(book_info(td))\n",
    "\n",
    "        # now be a good citizen and respect the robots.txt!\n",
    "        sleep(30)\n",
    "\n",
    "    return books\n",
    "\n",
    "def get_year(book):\n",
    "    \"\"\"book[\"date\"] looks like 'November 2014' so we need to \n",
    "    split on the space and then take the second piece\"\"\"\n",
    "    return int(book[\"date\"].split()[1])\n",
    "\n",
    "def plot_years(plt, books):\n",
    "    # 2014 is the last complete year of data (when I ran this)\n",
    "    year_counts = Counter(get_year(book) for book in books\n",
    "                          if get_year(book) <= 2014)\n",
    "\n",
    "    years = sorted(year_counts)\n",
    "    book_counts = [year_counts[year] for year in x]\n",
    "    plt.bar([x - 0.5 for x in years], book_counts)\n",
    "    plt.xlabel(\"year\")\n",
    "    plt.ylabel(\"# of data books\")\n",
    "    plt.title(\"Data is Big!\")\n",
    "    plt.show()\n",
    "\n",
    "##\n",
    "# \n",
    "# APIs\n",
    "#\n",
    "##\n",
    "\n",
    "endpoint = \"https://api.github.com/users/joelgrus/repos\"\n",
    "\n",
    "repos = json.loads(requests.get(endpoint).text)\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)\n",
    "\n",
    "####\n",
    "#\n",
    "# Twitter\n",
    "#\n",
    "####\n",
    "\n",
    "from twython import Twython\n",
    "\n",
    "# fill these in if you want to use the code\n",
    "CONSUMER_KEY = \"\"\n",
    "CONSUMER_SECRET = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "ACCESS_TOKEN_SECRET = \"\"\n",
    "\n",
    "def call_twitter_search_api():\n",
    "\n",
    "    twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "    # search for tweets containing the phrase \"data science\"\n",
    "    for status in twitter.search(q='\"data science\"')[\"statuses\"]:\n",
    "        user = status[\"user\"][\"screen_name\"].encode('utf-8')\n",
    "        text = status[\"text\"].encode('utf-8')\n",
    "        print(user, \":\", text)\n",
    "        print\n",
    "\n",
    "from twython import TwythonStreamer\n",
    "\n",
    "# appending data to a global variable is pretty poor form\n",
    "# but it makes the example much simpler\n",
    "tweets = [] \n",
    "\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    \"\"\"our own subclass of TwythonStreamer that specifies\n",
    "    how to interact with the stream\"\"\"\n",
    "\n",
    "    def on_success(self, data):\n",
    "        \"\"\"what do we do when twitter sends us data?\n",
    "        here data will be a Python object representing a tweet\"\"\"\n",
    "\n",
    "        # only want to collect English-language tweets\n",
    "        if data['lang'] == 'en':\n",
    "            tweets.append(data)\n",
    "\n",
    "        # stop when we've collected enough\n",
    "        if len(tweets) >= 1000:\n",
    "            self.disconnect()\n",
    "\n",
    "    def on_error(self, status_code, data):\n",
    "        print(status_code, data)\n",
    "        self.disconnect()\n",
    "\n",
    "def call_twitter_streaming_api():\n",
    "    stream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET, \n",
    "                        ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "    # starts consuming public statuses that contain the keyword 'data'\n",
    "    stream.statuses.filter(track='data')\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def process(date, symbol, price):\n",
    "        print(date, symbol, price)\n",
    "\n",
    "    print(\"tab delimited stock prices:\")\n",
    "\n",
    "    with open('tab_delimited_stock_prices.txt', 'rb') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            date = row[0]\n",
    "            symbol = row[1]\n",
    "            closing_price = float(row[2])\n",
    "            process(date, symbol, closing_price)\n",
    "\n",
    "    print\n",
    "\n",
    "    print(\"colon delimited stock prices:\")\n",
    "\n",
    "    with open('colon_delimited_stock_prices.txt', 'rb') as f:\n",
    "        reader = csv.DictReader(f, delimiter=':')\n",
    "        for row in reader:\n",
    "            date = row[\"date\"]\n",
    "            symbol = row[\"symbol\"]\n",
    "            closing_price = float(row[\"closing_price\"])\n",
    "            process(date, symbol, closing_price)\n",
    "\n",
    "    print\n",
    "\n",
    "    print(\"writing out comma_delimited_stock_prices.txt\")\n",
    "\n",
    "    today_prices = { 'AAPL' : 90.91, 'MSFT' : 41.68, 'FB' : 64.5 }\n",
    "\n",
    "    with open('comma_delimited_stock_prices.txt','wb') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        for stock, price in today_prices.items():\n",
    "            writer.writerow([stock, price])\n",
    "\n",
    "    print(\"BeautifulSoup\")\n",
    "    html = requests.get(\"http://www.example.com\").text\n",
    "    soup = BeautifulSoup(html)\n",
    "    print(soup)\n",
    "    print\n",
    "\n",
    "    print(\"parsing json\")\n",
    "\n",
    "    serialized = \"\"\"{ \"title\" : \"Data Science Book\",\n",
    "                      \"author\" : \"Joel Grus\",\n",
    "                      \"publicationYear\" : 2014,\n",
    "                      \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\n",
    "\n",
    "    # parse the JSON to create a Python object\n",
    "    deserialized = json.loads(serialized)\n",
    "    if \"data science\" in deserialized[\"topics\"]:\n",
    "        print(deserialized )\n",
    "\n",
    "    print\n",
    "\n",
    "    print(\"GitHub API\")\n",
    "    print(\"dates\", dates)\n",
    "    print(\"month_counts\", month_counts)\n",
    "    print(\"weekday_count\", weekday_counts)\n",
    "\n",
    "    last_5_repositories = sorted(repos,\n",
    "                                 key=lambda r: r[\"created_at\"],\n",
    "                                 reverse=True)[:5]\n",
    "\n",
    "    print(\"last five languages\", [repo[\"language\"] \n",
    "                                  for repo in last_5_repositories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
