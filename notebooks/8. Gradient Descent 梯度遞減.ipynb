{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. [Gradient Descent 梯度遞減](https://github.com/joelgrus/data-science-from-scratch/blob/master/code/gradient_descent.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter\n",
    "from linear_algebra import distance, vector_subtract, scalar_multiply\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_of_squares(v):\n",
    "    \"\"\"computes the sum of squared elements in v\"\"\"\n",
    "    return sum(v_i ** 2 for v_i in v)\n",
    "\n",
    "a = [1, 2]\n",
    "sum_of_squares(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.000999999999479"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "difference_quotient(f, 3, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nR2jhK6WUj9DCV0opH/H/Ad+8/pK4Rlj2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f483b729630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_estimated_derivative(h=0.00001):\n",
    "\n",
    "    def square(x):\n",
    "        return x * x\n",
    "\n",
    "    def derivative(x):\n",
    "        return 2 * x\n",
    "\n",
    "    derivative_estimate = lambda x: difference_quotient(square, x, h=h)\n",
    "\n",
    "    # plot to show they're basically the same\n",
    "#     import matplotlib.pyplot as plt\n",
    "    x = np.linspace(-10, 10, 100)\n",
    "    plt.plot(x, list(map(derivative, x)), label = 'derivative')           # red  x\n",
    "    plt.plot(x, list(map(derivative_estimate, x)), label = 'derivative_estimate')  # blue +\n",
    "    plt.legend()\n",
    "    plt.show()                                      # purple *, hopefully\n",
    "    \n",
    "plot_estimated_derivative()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.000999999999699"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def partial_difference_quotient(f, v, i, h):\n",
    "    # add h to just the i-th element of v\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "         for j, v_j in enumerate(v)]         \n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "a = [1, 2]\n",
    "partial_difference_quotient(sum_of_squares, a, 1, 0.001)  # a 點 在 分量 1 上的 difference quotient, 函數是 sum_of_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0009999999999195, 4.000999999999699]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i, _ in enumerate(v)] \n",
    "\n",
    "a = [1, 2]\n",
    "estimate_gradient(sum_of_squares, a, 0.001)  # sum_of_squares 在點 a 位置上的偏微分 gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_of_squares_gradient(v): \n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "sum_of_squares_gradient(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9799900000000008, 1.959990000000003]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "a = [1, 2]\n",
    "gradient = estimate_gradient(sum_of_squares, a, 0.001)\n",
    "step(a, gradient, -0.01)\n",
    "\n",
    "# for _ in range(10000):\n",
    "#     gradient = estimate_gradient(sum_of_squares, a, 0.001)\n",
    "#     a = step(a, gradient, -0.01)    \n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe(f):\n",
    "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')         # this means \"infinity\" in Python\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# \n",
    "# minimize / maximize batch\n",
    "#\n",
    "#\n",
    "\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
    "    \n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    \n",
    "    theta = theta_0                           # set theta to initial value\n",
    "    target_fn = safe(target_fn)               # safe version of target_fn\n",
    "    value = target_fn(theta)                  # value we're minimizing\n",
    "    \n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)  \n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                       for step_size in step_sizes]\n",
    "                   \n",
    "        # choose the one that minimizes the error function        \n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        \n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negate(f):\n",
    "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                          negate_all(gradient_fn),\n",
    "                          theta_0, \n",
    "                          tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# minimize / maximize stochastic\n",
    "#\n",
    "\n",
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
    "    random.shuffle(indexes)                    # shuffle them\n",
    "    for i in indexes:                          # return the data in that order\n",
    "        yield data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x, y 中的每一對元素，和 theta 經過運算，求能使 target_fn(x_i, y_i, theta) 得到最小值的 theta\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "\n",
    "    data = zip(x, y)\n",
    "    theta = theta_0                             # initial guess\n",
    "    alpha = alpha_0                             # initial step size\n",
    "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    # if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data)  # 將每個資料點所預測的誤差累加起來，作為整組資料誤差的預測值\n",
    "\n",
    "        if value < min_value:\n",
    "            # if we've found a new minimum, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "\n",
    "        # and take a gradient step for each of the data points\n",
    "        # \n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)  # 求特定資料點上的 gradient\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))  # 調整 theta\n",
    "            \n",
    "    return min_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn),\n",
    "                               negate_all(gradient_fn),\n",
    "                               x, y, theta_0, alpha_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the gradient\n",
      "minimum v [4.163601529674763e-06, -1.8504895687443374e-06, -1.8504895687443374e-06]\n",
      "minimum value 2.418420098597323e-11\n",
      "\n",
      "using minimize_batch\n",
      "minimum v [4.163601529674763e-06, -1.8504895687443374e-06, -1.8504895687443374e-06]\n",
      "minimum value 2.418420098597323e-11\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"using the gradient\")\n",
    "\n",
    "    v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "    tolerance = 0.0000001\n",
    "\n",
    "    while True:\n",
    "        #print(v, sum_of_squares(v)\n",
    "        gradient = sum_of_squares_gradient(v)   # compute the gradient at v\n",
    "        next_v = step(v, gradient, -0.01)       # take a negative gradient step\n",
    "        if distance(next_v, v) < tolerance:     # stop if we're converging\n",
    "            break\n",
    "        v = next_v                              # continue if we're not\n",
    "\n",
    "    print(\"minimum v\", v)\n",
    "    print(\"minimum value\", sum_of_squares(v))\n",
    "    print()\n",
    " \n",
    "    \n",
    "    print(\"using minimize_batch\") \n",
    "\n",
    "    v = minimize_batch(sum_of_squares, sum_of_squares_gradient, v)  # 使用 minimize_batch 求最小值\n",
    "\n",
    "    print(\"minimum v\", v)\n",
    "    print(\"minimum value\", sum_of_squares(v))    \n",
    " \n",
    "    \n",
    "#     print(\"using minimize_stochastic\") \n",
    "\n",
    "#     v = minimize_stochastic(sum_of_squares, sum_of_squares_gradient, v)  # 使用 minimize_stochastic 求最小值\n",
    "\n",
    "#     print(\"minimum v\", v)\n",
    "#     print(\"minimum value\", sum_of_squares(v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
